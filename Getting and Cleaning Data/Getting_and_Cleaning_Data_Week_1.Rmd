---
title: "Getting and Cleaning Data Week 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r packages, include=FALSE }
library(readxl)
library(XML)
library(jsonlite)
library(data.table)
library(dplyr)
```


# Course Description
Before you can work with data you have to get some. This course will cover the basic ways that data can be obtained. The course will cover obtaining data from the web, from APIs, and from colleagues in various formats including raw text files, binary files, and databases. It will also cover the basics of data cleaning and how to make data tidy. Tidy data dramatically speed downstream data analysis tasks. The course will also cover the components of a complete data set including raw data, processing instructions, codebooks, and processed data. The course will cover the basics needed for collecting, cleaning, and sharing data.

 - Data Collection  
  -- Raw files (.csv, .xlsx)  
  -- Databases (mySQL)  
  -- APIs  
 - Data Formats  
  -- Flat files (.csv, .txt)  
  -- XML  
  -- JSON  
 - Making Data Tidy  
 - Distributing data  
 - Scripting for data cleaning  
 
# Obtaining Data Motivation  
 - Basic ideas behind getting data ready for analysis  
  -- Finding and extracting raw data  
  -- tidy data principles and how to make data tidy  
  -- Practical implementation through a range of R packages  
 - Prerequisite courses:  
  -- The Data Scientist's Toolbox  
  -- R Programming  
 - Other useful courses:  
  -- Exploratory Analysis  
  -- Reporting Data and Reproducible Research  

 **Goal of this course:**  
 Focus on the first three stages:  
 **raw data** -> **Processing script** -> **tidy data** -> data analysis -> data communication  
 
# Raw and Processed Data  
 *Raw data* can be different, according to who you're speaking to.  
 "Data are values of qualitative or quantitative variables, belonging to a set of items."  

 **Raw Data**  
 
 - Original source of the data  
 - Often hard to use for data analyses  
 - Data analysis *includes* processing  
 - Raw data may only need to be processed once  
 
 **Processed Data**  
 
 - Data that is ready for analysis  
 - Processing can include merging, subsetting, transforming, etc.  
 - There may be standards for processing, depending on the field your work in.  
 - **All steps should be recorded.**  

# Components of tidy data  
Four things you should have when you finished going from the raw data to a tidy data set:  

 - the raw data.  
 - a tidy data set  
 - a code book describing each variable and its values in the tidy data set (often called metadata)  
 - an explicit and exact recipe you used to go from 1 to 2 and 3. (in this case it will be recorded as an R script)  

## The Raw Data should be the rawest form of the data that you had access to.  
 - You ran no software on the data.  
 - You did not manipulate any numbers in the data set  
 - you did not remove any data from the data set
 - you did not summarise the data set in any way

## The tidy data is your objective  
 - each variable you measure is in one column  
 - each observation should be in a different row  
 - there should be one table for each "kind" of variable (eg: data from twitter, fb, etc, one table for each)  
 - if there are multiple tables, they should include a column in the table that allows them to be linked together.  

 *include variable names if possible, and make them human-readable.*
 *in general, data should be saved in one file per table*
 
## The code book  
 - Information about the variables in the data set, not contained in the tidy data.  
 - Information about the summary choices made. (eg. mean or median)  
 - Information about the experimental study design used.  
 
 *often written in Word or text file, or Rmarkdown*  
 *include a section called "Study design" that has a thorough description of how you collected the data*  
 *include a section called "code book" that describes all variables and units*  

**Instruction list**  

 - ideally a computer script (in R or python)  
 - the input for the script is the raw data  
 - the output is the processed, tidy data  
 - there are no parameters to the script -> exact recipe  
 *Needs to be reproducible*

In some cases, it will not be possible to script every step. In that case you should provide instructions like:  

 - Step 1: take the raw file, run version 3.1.2 of *summarize software* with parameters a=1, b=2, c=3  
 - step 2: run software separately for each sample  
 - setp 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set.  
*very detailed instructions.* **REPRODUCIBILITY**  

# Downloading files with R  
You might use R to download files, so that the downloading process is included in the processing script.  

 - A basic component of working with data is knowing your working directory.  
 - the two main commands are `getwd()` and `setwd()`.  
 - Be aware of the relative versus absolute paths:  
  -- **relative** `setwd("../")`  
  -- **absolute** `setwd("C:/Users/RudyR/OneDrive/Desktop")`  
 - Important difference in Windows: `setwd(C:\\users\\RudyR\\Desktop)`  
 
## Checking for and creating directories  
 - `file.exists("directoryName")`  
 - `dir.create("directoryName")`  
```{r}
if(!file.exists("../data")) {
    dir.create("../data")
}  ## creates a folder called data, if it doesn't already exist.
```  

## Getting data from the internet  
 - `download.file()` command.  
 - Even if you could do this by hand, using the command helps with the reproducibility.  
 - important parameters are `url`, `destfile` and `method`.  
 - useful for downloading tab-delimited, csv and other files.  

```{r Getting data from the internet, error=TRUE}
fileURL <- "https://www.stats.govt.nz/assets/Uploads/Household-expenditure-statistics/Household-expenditure-statistics-Year-ended-June-2019/Download-data/detailed-household-expenditure-year-ended-June-2019-csv.csv"
destinationfile <- "C:/Users/RudyR/Desktop/R Projects/Rudys_First_Project/data/HouseholdExpenditure2019.csv"
## download.file(fileURL, destfile = destinationfile)
list.files("C:/Users/RudyR/Desktop/R Projects/Rudys_First_Project/data/")
dateDownloaded <- date()
dateDownloaded
```  

 - method `curl` needs to be specified for https websites when downloading on a Mac. Not needed on windows.  
 - it's important to keep track of the date in which it was downloaded, as the file online could be updated.  
 
 **Notes**  
 
 - if the url starts with *http*, you can use download.file()  
 - if it starts with *https*, on windows you may be ok.  
 - if it starts with *https*, on Mac you need to set `method = "curl"`  
 - if the file is big, this might take a while. You may set it up so the file doesn't re-download every time you run the code.  
 - be sure to record the date when you downloaded it.  
 
# Reading local files  
 - repeat of content covered in R programming course  
```{r Reading local files 1}
str(read.table)
```  

 - `read.table()` is the main function, but requires a lot of parameters.  
 - there are faster methods  
 - reads the data into RAM, can cause problems for large sets.  
 - related to `read.csv()` and `read.csv2()`  
 
```{r reading local files 2}
destinationfile <- "C:/Users/RudyR/Desktop/R Projects/Rudys_First_Project/data/HouseholdExpenditure2019.csv"
ExpenditureData <- read.table(destinationfile, sep = ",", header = TRUE)
head(ExpenditureData)
```  

 - `read.csv()` has defaults `sep = ","` and `header = TRUE`.  
 - other parameters:  
  -- `quote` to tell R about quoted values. `quote=""` means no quotes.  
  -- `na.strings` sets the character that represents a missing value.  
  -- `nrows` how many rows to read the file.  
  -- `skip` how many lines to skip before starting to read.  

Quotation marks ' or " in data values can cause trouble for reading the data. Setting `quote=""` often resolves this.  

# Reading Excel Files  
Use packages to read excel files, such as *xlsx* (requires java), or *readxl* package. 
```{r reading excel files}
## library(readxl)
str(read_xlsx)  ## many optional arguments
cameras <- read_xlsx("../data/cameras.xlsx")
head(cameras)

 ## other 
 ## write.xlsx function available in xlsx package
 ## 
 ## XLConnect package has even more options for writing and manipulating
```  

 - *xlsx* package include options to read only specific columns. *readxl* can only select by rows.  
 - `write.xlsx` function available in *xlsx* package  
 - `read.xlsx2` is faster in large files.  
 - *XLConnect* package has even more options for writing and manipulating Excel files. Read the vignette if starting to use that package.  
 - in general it is advised to store your data in either a database, or in `.csv` or tab separated files `.tab` or `.txt`, as they are easier to distribute.  
 
# Reading XML  
 - *Extensible markup language*  
 - used to store stuctured data  
 - particularly used in internet applications  
  -- Markup: labels that give the text structure  
  -- Content: the actual text of the document  

  **Tags, elements and attributes**  
  
 - Tags are general labels. E.g.: `<section>` and `</section>` to start and end a section, or `<line-break />` is an empty tag.  
 - Elements are specific examples of tags. E.g.: `<Greeting>` *Hello* `</Greeting>`  
 - Attributes are components of the label. E.g.: `<img src="jeff.jpg" alt="instructor" />  
 - Very similar to html  

  *Using the XML package*
```{r reading XML files 1}
## library(XML)
fileURL <- "http://www.w3schools.com/xml/simple.xml"
## download.file(fileURL, destfile = "../data/simple.xml")
doc <- xmlTreeParse("../data/simple.xml", useInternalNodes = TRUE) 
    ## loads the document into RAM in a way R can access it. Still a 
    ## structured object, though.

rootNode <- xmlRoot(doc)  ## accesses that particular element
xmlName(rootNode)  ## accesses the name

names(rootNode) ## accesses all names in the structure

rootNode[[1]]  ## access elements of the structured object, as if it's a list.

rootNode[[1]][[1]]  ## first subcomponent of the first subcomponent.

xmlSApply(rootNode, xmlValue) 
    ## similar to sapply, loops through the parseXML document and applies the
    ## function xmlValue. Returns all the values separated only at the top level 
    ## node: "food".
```  

  *Using the `XPath`, included in XML package*  
Requires a whole new language, as well as learning XML, but a superficial understanding can already yield good results.  
More information on [this link](http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf).  

 - `/node` Top level node  
 - `//node` node at any level  
 - `node[@attr-name]` Node with an attribute name  
 - `node[@attr-name='bob']` Node with attribute name attrname='bob'  

```{r reading XML files 2}
## applies xmlValue to the nodes called "name"
xpathSApply(rootNode, "//name", xmlValue) 

 ## turning it into a matrix
y <- xpathApply(rootNode, 
                c("//name","//price","//description","//calories"), 
                xmlValue)
matrix(y,nrow = 4, ncol = 5)
```  

## Reading an HTML file  
```{r reading HTML files}
fileUrl <- "https://www.espn.com/nfl/team/schedule/_/name/bal/year/2016"
## download.file(fileUrl, destfile = "../data/ravens.html")
doc <- htmlTreeParse("../data/ravens.html", useInternal=TRUE)
scores <- xpathSApply(doc, "//span[@class='ml4']", xmlValue)
scores

## getting the team names is harder as the <span> tag doesn't have any 
## attributes to call it from. But you can select a more generic tag and build 
## a matrix, then subsetting.
a <-  xpathSApply(doc, "//a[@class='AnchorLink']", xmlValue)
a ## all "a" tags with class = "AnchorLink", gives us the main table of results
matx <- matrix(a[1:108], nrow = 18, ncol = 6, byrow = TRUE)
colnames(matx) <- c("", "Opponents", "Result", "HI PASS", "HI RUSH", "HI REC")
matx ## Column 1 is empty
matx[ ,2] ## Opponent names
```  

Read also the XML package tutorials, in the *Reading XML guides* folder.

# Reading JSON files  
 - **Javascript Object Notation**  
 - Lightweight way of storing data  
 - commonly format for data from application programming interfaces.  
 - Similar to XML structure, but different sintax.  
 - Data is stored as :  
  -- Numbers (double)  
  -- Strings (double quoted)  
  -- Boolean  
  -- array (comma separated enclosed in quare brackets)  
  -- object (unordered, comma separated collection of key:value pairs in curly brackets)  

best place to start is wikipedia.  

Use the `jsonlite` package  
```{r reading JSON files}
## library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
 ## object is a structured data frame. 
names(jsonData) ## top level variables
names(jsonData$owner) ## within the element "owner" was stored another data 
                      ## frame with these top level names
```  

## Writing JSON files  
```{r writing JSON files}
## Using the Iris dataset as an example
myjson <- toJSON(iris, pretty = TRUE) 
## pretty = TRUE gives it readable indentation.
class(myjson)
iris2 <- fromJSON(myjson) ## getting it back as a data frame
head(iris2)
head(iris)  ## data frame is identical to the original one
```  
[This link](http://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder) contains a good tutorial on jsonlight. Also, check the jsonlite vignette and www.json.org for further resources.  

# The data.table Package  
Often faster and more memory efficient than data frames. All functions that accept data.frame work on data.table. Faster at subsetting, group and updating variables. Updated often.  
Requires its own syntax.  
```{r using data.table}
## library(data.table)
## can be used in the same way as data.frame
DF <- data.frame(x=round(rnorm(9),3),
                 y=rep(c("a","b","c"),each=3),
                 z=round(rnorm(9),3))
head(DF,3)
DT <- data.table(x=round(rnorm(9),3),
                 y=rep(c("a","b","c"),each=3),
                 z=round(rnorm(9),3))
head(DT,3)

tables() ## allows you to check all data tables in the memory
```  

## Subsetting a data table  
```{r Subsetting a data table 1}
## similar to data.frame
DT[2,] ## subset rows by index
DT[DT$y=="b",] ## subset rows by logical vector

## different:
## when using only one index, it subsets based on rows instead of columns
DT[c(2,3)]
DF[c(2,3)]
```  

**Subsetting columns is different:**  

 - the argument you pass after the comma is called an "expression".  
 - In R an expression is a collection of statements enclosed in curly brackets.  
 - example:
```{r Subsetting a data table 2}
{
  x <- 1
  y <- 2
} ## one expression
k <- {print(10);5} ## print is 10 but value is 5
print(k)

## So for example, instead of putting an index in the subsetting, you can pass 
## a list of functions you want to perform.
DT[,list(mean(x),sum(z))]
DT[,table(y)]
```  
 
## Adding new columns  
```{r Adding columns to data tables 1}
DT[, w:=round(z^2,3)] ## use := to add the variable w to the data frame.
## Doing this with data frame can be very memory consuming, but it's not a 
## problem in data.table
## 
## But be careful if making a copy of the data table, as changes to the 
## original will change the copies, too.
DT2 <- DT
DT[ , y:= 2]
head(DT2,3) ## DT2 is changed with it.

## overcome this by using the copy() function
DT3 <- copy(DT)
DT[ , y:= 4] ## change y to 4
head(DT3,3) ## not changed
```

### Multiple step functions to create new variables  
 - each statement in an expression is followed by a ";".  
```{r Adding columns to data tables 2}
DT[ ,m:= {tmp <- (x+z); tmp2 <- log2(tmp+5); round(tmp2,2)}]
## assign temporary variable tmp, then use it.
head(DT, 3)
```  

### "plyr" like operations  
 - plyr is an R package that makes it simple to split data apart, do stuff to it, and mash it back together.  
```{r Other operations in data.table}
DT[ ,a:=x>0] ## column a is logical
head(DT,3)
DT[, b:= mean(x+w),by=a] ## mean of rows where a == True is separated from 
                         ## mean of rows where a == False
head(DT,3) ## this is pretty cool!
```  

### Special variables in data.table  
 - `.N` An integer, length 1, containing the number of rows  
  -- contains the number of times that a specific group appears  
```{r Special variables in data.table}
set.seed(123)
DT2 <- data.table(x=sample(letters[1:3], 1E5, replace = TRUE))
DT2[ , .N, by=x] ## count number of times / rows by x variable
## calculates it faster than table(DT2$x)
```  
 
### Keys  
Setting a key allows you to sort and subsett the data table much faster than you'd be able to on a data frame.  
```{r Keys in data.table}
DT2 <- data.table(x=rep(letters[1:3],each=100), y=round(rnorm(300),3))
setkey(DT2,x) ## column x is set as the key
head(DT2['a'],6) ## subsetting automatically knows to search for the key
```  

### Joining data tables  
 - Use keys to facilitate joins  
```{r Joining data tables}
DT1 <- data.table(x=c("a","a","b","dt1"), y=1:4)
DT2 <- data.table(x=c("a","b","dt2"), z=5:7)
DT1
DT2

setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)  ## merge function uses the keys to organise the rows. Rows 
##unique to each data table are excluded.
## much faster than merging with data frame.
```  

### Faster reading from file  
 - `fread` function is provided by the data.table package.  
 - stands for "fast read"  
 - substitute for `read.table`
```{r reading files is faster with data.table}
big_df <- data.frame(x=round(rnorm(1E6),3),y=round(rnorm(1E6),3))
head(big_df, 3)

file <- tempfile()  ## location of a new temporary file in temp files folder.
write.table(big_df, file=file, row.names = FALSE, col.names = TRUE, 
            sep = "\t", quote = FALSE) ## writes the data frame in the temp file
system.time(fread(file))  ## time required to read the file
system.time(read.table(file, header=TRUE, sep="\t"))
```  

New functionalities are frequently being added to the data.table package.  

 - The latest version can be found [here](https://r-forge.r-project.org/scm/viewvc.php/pkg/NEWS?view=markup&root=datatable)  
 - [Here](http://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table) is a list of differences between `data.table` and `data.frame`  
 - Credit to [Raphael Gottardo and Kevin Ushey](https://github.com/raphg/Biostat-578/blob/master/Advanced_data_manipulation.Rpres)

# Swirl
Install Getting and Cleaning Data course with the command `install_from_swirl("Getting and Cleaning Data")`.  

## Manipulating Data with `dplyr` package  
```{r dplyr setup, include=FALSE}
## library(dplyr)
set.seed(17)
x <- 50
datfra <- data.frame(Name = paste(sample(LETTERS, x, replace = TRUE),
                                   sample(letters, x, replace = TRUE),
                                   sample(letters, x, replace = TRUE),
                                   sample(letters, x, replace = TRUE), 
                                   sep = ""), 
                     Height = round(rnorm(x, mean = 1.6, sd = 0.2),2), 
                     Time = round(rnorm(x, mean = 30, sd = 8),1), 
                     Confidence = round(runif(x,0,10),0),
                     Age = round(runif(x,18,65),0),
                     Score = round(rnorm(x, 6, 2),1),
                     IQ = round(rnorm(x, 100, 10),0),
                     Date = rep(Sys.Date(), times = x),
                     Country = sample(c("NZ","US","UK","AUS","CAN"), 
                                      size = x, replace = TRUE))
## convert data frame into a 'data frame tbl' format so dplyr can manipulate it
mytibble <- tibble(datfra)

newcols <- data.frame(Test_Version = rep(1.1, times = x),
                      Weather = sample(c("Sunny","Rain","Cloudy","Windy"),
                                       x, replace = TRUE),
                      Gender = sample(c("Male","Female","Male","Female","Male",
                                        "Female","Other"),x, replace = TRUE),
                      Blood_Type = sample(c("A","B","AB","0"),
                                          x, replace = TRUE))
newcols <- tibble(newcols)
```  
```{r intro to dplyr}
## The  advantage to using a tbl_df over a regular data frame is the printing.
mytibble
```  
First, we are shown the class and dimensions of the dataset. Just below that, we get a preview of the data. Instead of attempting to print the entire dataset, dplyr just shows us the first 10 rows of data and only as many columns as fit neatly in our console. At the bottom, we see the names and classes for any variables that didn't fit on our screen.  
"The dplyr philosophy is to have small functions that each do one thing well." Specifically, dplyr supplies five 'verbs' that cover most fundamental data manipulation tasks: `select()`, `filter()`, `arrange()`, `mutate()`, and `summarize()`.  

### `select()`  
Select (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right). 
```{r dplyr select 1}
y <- select(mytibble, Name, Country, Score)
## can rearrange the order of columns, no need to type datfratbl$Name, etc.
head(y,5) 
```  
You can also use predicate functions like `is.numeric` to select variables based on their properties.  
Tidyverse selections implement a dialect of R where operators make it easy to select variables:  

 - `:` for selecting a range of consecutive variables.  
 - `-` to omit a column.  
 - `!` for taking the complement of a set of variables.  
 - `&` and | for selecting the intersection or the union of two sets of variables.  
 - `c()` for combining selections.  
 - `everything()` Matches all variables.  
 - `last_col()` Select last variable, possibly with an offset.  
 - `starts_with()` Starts with a prefix.  
 - `ends_with()` Ends with a suffix.  
 - `contains()` Contains a literal string.  
 - `matches()` Matches a regular expression.  
 - `num_range()` Matches a numerical range like x01, x02, x03.  
 - `all_of()` Matches variable names in a character vector. All names must be present, otherwise an out-of-bounds error is thrown.  
 - `any_of()` Same as all_of(), except that no error is thrown for names that don't exist.  
 - `where()` Applies a function to all variables and selects those for which the function returns TRUE.
```{r dplyr select 2}
y <- select(mytibble, Name:Confidence) ## using ":" to select columns
head(y,5)
y <- select(mytibble, -IQ)  ## using "-" to omit a column
head(y,5)
y <- select(mytibble, -(Score:Country))  ## mixing different strategies
head(y,5)
```  

### `filter()`  
The `filter()` function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of `TRUE` for all conditions. Note that when a condition evaluates to `NA` the row will be dropped, unlike base subsetting with `[`.  
```{r dplyr filter}
y <- filter(mytibble, Height >= 1.8)  ## no need to use datfratbl$Height
head(y,5)
```


### `arrange()`  
Orders the rows of a data frame by the values of selected columns.  
```{r dplyr arrange}
y <- arrange(mytibble, Name)
head(y,5)
y <- arrange(mytibble, Country, Name) 
## arrange by Country, and on identical groups, arrange by name
head(y,5)
```  

### `mutate()`  
`mutate()` adds new variables and preserves existing ones; `transmute()` adds new variables and drops existing ones. New variables overwrite existing variables of the same name. Variables can be removed by setting their value to `NULL`.  
```{r dplyr mutate}
y <- mutate(mytibble, Correct_Time = Time + 10) ## add 10 to Time in new column
head(select(y, -(Confidence:Date)),5)
y <- mutate(mytibble, Date = NULL) ## do not display Date column
head(y,5)

## merge data frames
head(newcols,5)
y <- mutate(select(mytibble, -IQ, -Confidence, -Date), newcols)
head(y,5)
```  

### `summarize()`  
`summarise() `creates a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified.  
`summarise()` and `summarize()` are synonyms.  
```{r dplyr summarize}
summarize(mytibble, mean(Time)) ## mean of the Time column
## this function becomes more useful once we introduce grouping to the 
## data frame
```

# Quiz Week 1  
*Question 1*
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:  
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv  
and load the data into R. The code book, describing the variable names is here:  
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf  
How many properties are worth $1,000,000 or more?  
```{r question 1}
housing <- read.csv("./Quiz week 1/getdata_data_ss06hid.csv")
VAL <- housing$VAL[!is.na(housing$VAL)]
sum(VAL == 24)
```  

*Question 2*  
Use the data you loaded from Question 1. Consider the variable FES in the code book. Which of the "tidy data" principles does this variable violate?  
```{r question 2}
## Numeric values in tidy data can not represent categories.
```  

*Question 3*  
Download the Excel spreadsheet on Natural Gas Aquisition Program here:  
 https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx  
Read rows 18-23 and columns 7-15 into R and assign the result to a variable called `dat`.  
What is the value of `sum(dat$Zip*dat$Ext,na.rm=T)`
```{r question 3}
dat <- read_xlsx("./Quiz week 1/getdata_data_DATA.gov_NGAP.xlsx", 
                 range = "G18:O23")
dat
sum(dat$Zip*dat$Ext,na.rm=T)
```  

*Question 4*  
Read the XML data on Baltimore restaurants from here:  
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml  
How many restaurants have zipcode 21231?  
```{r question 4}
rest <- xmlTreeParse("./Quiz week 1/getdata_data_restaurants.xml", 
                     useInternalNodes = TRUE)
datrest <- xmlRoot(rest)
zipcode <- xpathApply(datrest, "//zipcode", xmlValue)
sum(zipcode == 21231)
```

*Question 5*  
The American Community Survey distributes downloadable data about United States communities. Download the 2006 microdata survey about housing for the state of Idaho using download.file() from here:  
https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv  
using the fread() command load the data into an R object called `DT`  
The following are ways to calculate the average value of the variable `pwgtp15`  
broken down by sex. Using the data.table package, which will deliver the fastest user time?  
```{r question 5, error=TRUE}
DT <- fread(file = "./Quiz week 1/getdata_data_ss06pid.csv")

system.time(DT[,mean(pwgtp15),by=SEX])
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT[DT$SEX==1,]$pwgtp15),
         mean(DT[DT$SEX==2,]$pwgtp15))

mean(DT$pwgtp15,by=DT$SEX) ## incorrect answer
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]  ## returns error
```

```{r unloading packages, include=FALSE}
detach("package:dplyr", unload=TRUE)
detach("package:data.table", unload=TRUE)
detach("package:jsonlite", unload=TRUE)
detach("package:XML", unload=TRUE)
detach("package:readxl", unload=TRUE)
```


